{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To understand optimize gradient descent, Checking gradients using gradient Checking\n",
    "Analytical processer and takes longer time to obtained approximate gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing python libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def activation_function(Z, typ):\n",
    "    if typ == 'sig':\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "        assert A.shape == Z.shape\n",
    "        return A\n",
    "    elif typ == 'tanh':\n",
    "        A = np.tanh(Z)\n",
    "        assert A.shape == Z.shape\n",
    "        return A\n",
    "    elif typ == 'relu':\n",
    "        A = np.maximum(0, Z)\n",
    "        assert A.shape == Z.shape\n",
    "        return A\n",
    "\n",
    "\n",
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    para_list=parameters.keys()\n",
    "    keys = []\n",
    "    count = 0\n",
    "    for key in para_list:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        keys = keys + [key] * new_vector.shape[0]\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    parameters[\"w1\"] = theta[: 20].reshape((5, 4))\n",
    "    parameters[\"b1\"] = theta[20: 25].reshape((5, 1))\n",
    "    parameters[\"w2\"] = theta[25: 40].reshape((3, 5))\n",
    "    parameters[\"b2\"] = theta[40: 43].reshape((3, 1))\n",
    "    parameters[\"w3\"] = theta[43: 46].reshape((1, 3))\n",
    "    parameters[\"b3\"] = theta[46: 47].reshape((1, 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    grad_list=gradients.keys()\n",
    "    start_letter1='w'\n",
    "    start_letter2='b'\n",
    "    grad_list=[x for x in grad_list if (x.startswith(start_letter1))]\n",
    "    print(grad_list)\n",
    "    count = 0\n",
    "    for key in [\"dw1\", \"db1\", \"dw2\", \"db2\", \"dw3\", \"db3\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running test cases to check if the NN is working or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "============================= test session starts =============================\nplatform win32 -- Python 3.7.4, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\nrootdir: C:\\Users\\VISWAMBHAR YASA\\ML\\Deep_learning\\DEEP_LEARNING\\Neural_network\\Deep_layered_NN\ncollected 4 items\n\ntest_NN_deep_layer.py ....                                               [100%]\n\n============================== 4 passed in 0.16s ==============================\n"
     ]
    }
   ],
   "source": [
    "!pytest test_NN_deep_layer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi layer Neural networt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['w3']\n",
    "    b3 = parameters['b3']\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = activation_function(Z1, 'relu')\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = activation_function(Z2, 'relu')\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = activation_function(Z3, 'sig')\n",
    "\n",
    "    # Cost\n",
    "    log_probs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1. / m * np.sum(log_probs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache\n",
    "\n",
    "def backward_propagation_n(X, Y, cache):\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1. / m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1. / m * np.dot(dZ2, A1.T) * 2\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1. / m * np.dot(dZ1, X.T)\n",
    "    db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dw3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dw2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dw1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def gradient_check(parameters, gradients, X, Y, epsilon=1e-7, print_msg=True):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \"\"\"\n",
    "\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    for i in range(num_parameters):\n",
    "        theta_plus=np.copy(parameters_values)\n",
    "        theta_plus[i][0]=theta_plus[i][0]+epsilon\n",
    "        J_plus[i], cache_plus=forward_propagation_n(X,Y,vector_to_dictionary(theta_plus))\n",
    "        \n",
    "\n",
    "        theta_minus=np.copy(parameters_values)\n",
    "        theta_minus[i][0]=theta_minus[i][0]-epsilon\n",
    "        J_minus[i], cache_minus=forward_propagation_n(X,Y,vector_to_dictionary(theta_minus))\n",
    "        \n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i])/(2 * epsilon)\n",
    "        \n",
    "\n",
    "    numerator = np.linalg.norm(grad - gradapprox)                                           \n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                        \n",
    "    difference = numerator/(denominator*1.0)\n",
    "    \n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4,3)\n",
    "y = np.array([1, 1, 0])\n",
    "w1 = np.random.randn(5,4) \n",
    "b1 = np.random.randn(5,1) \n",
    "w2 = np.random.randn(3,5) \n",
    "b2 = np.random.randn(3,1) \n",
    "w3 = np.random.randn(1,3) \n",
    "b3 = np.random.randn(1,1) \n",
    "parameters = {'w1': w1,\n",
    "                  'b1': b1,\n",
    "                  'w2': w2,\n",
    "                  'b2': b2,\n",
    "                  'w3': w3,\n",
    "                  'b3': b3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n\u001b[93mThere is a mistake in the backward propagation! difference = 0.28509315677616237\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "AL, fw_cache=forward_propagation_n(x,y, parameters)\n",
    "gradient=backward_propagation_n(x, y, fw_cache)\n",
    "diff=gradient_check(parameters, gradient, x, y, epsilon=1e-7, print_msg=True)"
   ]
  },
  {
   "source": [
    "### Using gradient check, gradients obtained are accurate or not"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python37464bitea317a4e8f5f425ab9c1f0cd775dc5b1",
   "display_name": "Python 3.7.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "fb638d1167cba67cdfb7dc0a9699f856a086081bb29198c415d9c987c4093cba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}